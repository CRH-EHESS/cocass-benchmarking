{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmarking\n",
    "This Notebook allows you to benchmark the different models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch available : True\n"
     ]
    }
   ],
   "source": [
    "from utils import init_notebook, benchmarking, predictions\n",
    "%aimport datasets, utils.predictions, utils.benchmarking,utils.data_cleaning\n",
    "from utils.predictions import * \n",
    "\n",
    "import sahi\n",
    "from sahi import AutoDetectionModel#, get_prediction\n",
    "import fiftyone as fo\n",
    "import torch\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "HOME = Path(os.getcwd()).parents[0]\n",
    "HOME\n",
    "\n",
    "print(\"torch available :\",torch.cuda.is_available())\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Importation\n",
    "For the benchmarking, we decide to import all the models using SAHI so that the objects handled are all sahi Autodetection models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### YOLO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name=\"yolass_aug_26ep\"\n",
    "\n",
    "yolo_detection_model = AutoDetectionModel.from_pretrained(\n",
    "    model_type=\"yolov8\",    # Model type (base model is yolov8 also for yolov10)\n",
    "    model_path=f\"../outputs/yolo/{model_name}.pt\",    # Path to the model weights\n",
    "    confidence_threshold=0.1,   # Confidence threshold\n",
    "    # The higher the confidence threshold, the more precise they are (but we do a filtering later)\n",
    "    device=\"cuda:0\",  # to use the GPU\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DETR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "09/04/2024 13:32:55 - INFO - timm.models._builder -   Loading pretrained weights from Hugging Face hub (timm/resnet101.a1h_in1k)\n",
      "09/04/2024 13:32:55 - INFO - timm.models._hub -   [timm/resnet101.a1h_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.\n",
      "09/04/2024 13:32:55 - INFO - timm.models._builder -   Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DetrForObjectDetection(\n",
       "  (model): DetrModel(\n",
       "    (backbone): DetrConvModel(\n",
       "      (conv_encoder): DetrConvEncoder(\n",
       "        (model): FeatureListNet(\n",
       "          (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "          (bn1): DetrFrozenBatchNorm2d()\n",
       "          (act1): ReLU(inplace=True)\n",
       "          (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "          (layer1): Sequential(\n",
       "            (0): Bottleneck(\n",
       "              (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn1): DetrFrozenBatchNorm2d()\n",
       "              (act1): ReLU(inplace=True)\n",
       "              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn2): DetrFrozenBatchNorm2d()\n",
       "              (drop_block): Identity()\n",
       "              (act2): ReLU(inplace=True)\n",
       "              (aa): Identity()\n",
       "              (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn3): DetrFrozenBatchNorm2d()\n",
       "              (act3): ReLU(inplace=True)\n",
       "              (downsample): Sequential(\n",
       "                (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                (1): DetrFrozenBatchNorm2d()\n",
       "              )\n",
       "            )\n",
       "            (1): Bottleneck(\n",
       "              (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn1): DetrFrozenBatchNorm2d()\n",
       "              (act1): ReLU(inplace=True)\n",
       "              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn2): DetrFrozenBatchNorm2d()\n",
       "              (drop_block): Identity()\n",
       "              (act2): ReLU(inplace=True)\n",
       "              (aa): Identity()\n",
       "              (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn3): DetrFrozenBatchNorm2d()\n",
       "              (act3): ReLU(inplace=True)\n",
       "            )\n",
       "            (2): Bottleneck(\n",
       "              (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn1): DetrFrozenBatchNorm2d()\n",
       "              (act1): ReLU(inplace=True)\n",
       "              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn2): DetrFrozenBatchNorm2d()\n",
       "              (drop_block): Identity()\n",
       "              (act2): ReLU(inplace=True)\n",
       "              (aa): Identity()\n",
       "              (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn3): DetrFrozenBatchNorm2d()\n",
       "              (act3): ReLU(inplace=True)\n",
       "            )\n",
       "          )\n",
       "          (layer2): Sequential(\n",
       "            (0): Bottleneck(\n",
       "              (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn1): DetrFrozenBatchNorm2d()\n",
       "              (act1): ReLU(inplace=True)\n",
       "              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "              (bn2): DetrFrozenBatchNorm2d()\n",
       "              (drop_block): Identity()\n",
       "              (act2): ReLU(inplace=True)\n",
       "              (aa): Identity()\n",
       "              (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn3): DetrFrozenBatchNorm2d()\n",
       "              (act3): ReLU(inplace=True)\n",
       "              (downsample): Sequential(\n",
       "                (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "                (1): DetrFrozenBatchNorm2d()\n",
       "              )\n",
       "            )\n",
       "            (1): Bottleneck(\n",
       "              (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn1): DetrFrozenBatchNorm2d()\n",
       "              (act1): ReLU(inplace=True)\n",
       "              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn2): DetrFrozenBatchNorm2d()\n",
       "              (drop_block): Identity()\n",
       "              (act2): ReLU(inplace=True)\n",
       "              (aa): Identity()\n",
       "              (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn3): DetrFrozenBatchNorm2d()\n",
       "              (act3): ReLU(inplace=True)\n",
       "            )\n",
       "            (2): Bottleneck(\n",
       "              (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn1): DetrFrozenBatchNorm2d()\n",
       "              (act1): ReLU(inplace=True)\n",
       "              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn2): DetrFrozenBatchNorm2d()\n",
       "              (drop_block): Identity()\n",
       "              (act2): ReLU(inplace=True)\n",
       "              (aa): Identity()\n",
       "              (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn3): DetrFrozenBatchNorm2d()\n",
       "              (act3): ReLU(inplace=True)\n",
       "            )\n",
       "            (3): Bottleneck(\n",
       "              (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn1): DetrFrozenBatchNorm2d()\n",
       "              (act1): ReLU(inplace=True)\n",
       "              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn2): DetrFrozenBatchNorm2d()\n",
       "              (drop_block): Identity()\n",
       "              (act2): ReLU(inplace=True)\n",
       "              (aa): Identity()\n",
       "              (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn3): DetrFrozenBatchNorm2d()\n",
       "              (act3): ReLU(inplace=True)\n",
       "            )\n",
       "          )\n",
       "          (layer3): Sequential(\n",
       "            (0): Bottleneck(\n",
       "              (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn1): DetrFrozenBatchNorm2d()\n",
       "              (act1): ReLU(inplace=True)\n",
       "              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "              (bn2): DetrFrozenBatchNorm2d()\n",
       "              (drop_block): Identity()\n",
       "              (act2): ReLU(inplace=True)\n",
       "              (aa): Identity()\n",
       "              (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn3): DetrFrozenBatchNorm2d()\n",
       "              (act3): ReLU(inplace=True)\n",
       "              (downsample): Sequential(\n",
       "                (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "                (1): DetrFrozenBatchNorm2d()\n",
       "              )\n",
       "            )\n",
       "            (1): Bottleneck(\n",
       "              (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn1): DetrFrozenBatchNorm2d()\n",
       "              (act1): ReLU(inplace=True)\n",
       "              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn2): DetrFrozenBatchNorm2d()\n",
       "              (drop_block): Identity()\n",
       "              (act2): ReLU(inplace=True)\n",
       "              (aa): Identity()\n",
       "              (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn3): DetrFrozenBatchNorm2d()\n",
       "              (act3): ReLU(inplace=True)\n",
       "            )\n",
       "            (2): Bottleneck(\n",
       "              (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn1): DetrFrozenBatchNorm2d()\n",
       "              (act1): ReLU(inplace=True)\n",
       "              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn2): DetrFrozenBatchNorm2d()\n",
       "              (drop_block): Identity()\n",
       "              (act2): ReLU(inplace=True)\n",
       "              (aa): Identity()\n",
       "              (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn3): DetrFrozenBatchNorm2d()\n",
       "              (act3): ReLU(inplace=True)\n",
       "            )\n",
       "            (3): Bottleneck(\n",
       "              (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn1): DetrFrozenBatchNorm2d()\n",
       "              (act1): ReLU(inplace=True)\n",
       "              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn2): DetrFrozenBatchNorm2d()\n",
       "              (drop_block): Identity()\n",
       "              (act2): ReLU(inplace=True)\n",
       "              (aa): Identity()\n",
       "              (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn3): DetrFrozenBatchNorm2d()\n",
       "              (act3): ReLU(inplace=True)\n",
       "            )\n",
       "            (4): Bottleneck(\n",
       "              (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn1): DetrFrozenBatchNorm2d()\n",
       "              (act1): ReLU(inplace=True)\n",
       "              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn2): DetrFrozenBatchNorm2d()\n",
       "              (drop_block): Identity()\n",
       "              (act2): ReLU(inplace=True)\n",
       "              (aa): Identity()\n",
       "              (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn3): DetrFrozenBatchNorm2d()\n",
       "              (act3): ReLU(inplace=True)\n",
       "            )\n",
       "            (5): Bottleneck(\n",
       "              (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn1): DetrFrozenBatchNorm2d()\n",
       "              (act1): ReLU(inplace=True)\n",
       "              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn2): DetrFrozenBatchNorm2d()\n",
       "              (drop_block): Identity()\n",
       "              (act2): ReLU(inplace=True)\n",
       "              (aa): Identity()\n",
       "              (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn3): DetrFrozenBatchNorm2d()\n",
       "              (act3): ReLU(inplace=True)\n",
       "            )\n",
       "            (6): Bottleneck(\n",
       "              (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn1): DetrFrozenBatchNorm2d()\n",
       "              (act1): ReLU(inplace=True)\n",
       "              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn2): DetrFrozenBatchNorm2d()\n",
       "              (drop_block): Identity()\n",
       "              (act2): ReLU(inplace=True)\n",
       "              (aa): Identity()\n",
       "              (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn3): DetrFrozenBatchNorm2d()\n",
       "              (act3): ReLU(inplace=True)\n",
       "            )\n",
       "            (7): Bottleneck(\n",
       "              (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn1): DetrFrozenBatchNorm2d()\n",
       "              (act1): ReLU(inplace=True)\n",
       "              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn2): DetrFrozenBatchNorm2d()\n",
       "              (drop_block): Identity()\n",
       "              (act2): ReLU(inplace=True)\n",
       "              (aa): Identity()\n",
       "              (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn3): DetrFrozenBatchNorm2d()\n",
       "              (act3): ReLU(inplace=True)\n",
       "            )\n",
       "            (8): Bottleneck(\n",
       "              (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn1): DetrFrozenBatchNorm2d()\n",
       "              (act1): ReLU(inplace=True)\n",
       "              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn2): DetrFrozenBatchNorm2d()\n",
       "              (drop_block): Identity()\n",
       "              (act2): ReLU(inplace=True)\n",
       "              (aa): Identity()\n",
       "              (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn3): DetrFrozenBatchNorm2d()\n",
       "              (act3): ReLU(inplace=True)\n",
       "            )\n",
       "            (9): Bottleneck(\n",
       "              (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn1): DetrFrozenBatchNorm2d()\n",
       "              (act1): ReLU(inplace=True)\n",
       "              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn2): DetrFrozenBatchNorm2d()\n",
       "              (drop_block): Identity()\n",
       "              (act2): ReLU(inplace=True)\n",
       "              (aa): Identity()\n",
       "              (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn3): DetrFrozenBatchNorm2d()\n",
       "              (act3): ReLU(inplace=True)\n",
       "            )\n",
       "            (10): Bottleneck(\n",
       "              (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn1): DetrFrozenBatchNorm2d()\n",
       "              (act1): ReLU(inplace=True)\n",
       "              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn2): DetrFrozenBatchNorm2d()\n",
       "              (drop_block): Identity()\n",
       "              (act2): ReLU(inplace=True)\n",
       "              (aa): Identity()\n",
       "              (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn3): DetrFrozenBatchNorm2d()\n",
       "              (act3): ReLU(inplace=True)\n",
       "            )\n",
       "            (11): Bottleneck(\n",
       "              (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn1): DetrFrozenBatchNorm2d()\n",
       "              (act1): ReLU(inplace=True)\n",
       "              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn2): DetrFrozenBatchNorm2d()\n",
       "              (drop_block): Identity()\n",
       "              (act2): ReLU(inplace=True)\n",
       "              (aa): Identity()\n",
       "              (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn3): DetrFrozenBatchNorm2d()\n",
       "              (act3): ReLU(inplace=True)\n",
       "            )\n",
       "            (12): Bottleneck(\n",
       "              (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn1): DetrFrozenBatchNorm2d()\n",
       "              (act1): ReLU(inplace=True)\n",
       "              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn2): DetrFrozenBatchNorm2d()\n",
       "              (drop_block): Identity()\n",
       "              (act2): ReLU(inplace=True)\n",
       "              (aa): Identity()\n",
       "              (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn3): DetrFrozenBatchNorm2d()\n",
       "              (act3): ReLU(inplace=True)\n",
       "            )\n",
       "            (13): Bottleneck(\n",
       "              (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn1): DetrFrozenBatchNorm2d()\n",
       "              (act1): ReLU(inplace=True)\n",
       "              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn2): DetrFrozenBatchNorm2d()\n",
       "              (drop_block): Identity()\n",
       "              (act2): ReLU(inplace=True)\n",
       "              (aa): Identity()\n",
       "              (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn3): DetrFrozenBatchNorm2d()\n",
       "              (act3): ReLU(inplace=True)\n",
       "            )\n",
       "            (14): Bottleneck(\n",
       "              (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn1): DetrFrozenBatchNorm2d()\n",
       "              (act1): ReLU(inplace=True)\n",
       "              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn2): DetrFrozenBatchNorm2d()\n",
       "              (drop_block): Identity()\n",
       "              (act2): ReLU(inplace=True)\n",
       "              (aa): Identity()\n",
       "              (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn3): DetrFrozenBatchNorm2d()\n",
       "              (act3): ReLU(inplace=True)\n",
       "            )\n",
       "            (15): Bottleneck(\n",
       "              (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn1): DetrFrozenBatchNorm2d()\n",
       "              (act1): ReLU(inplace=True)\n",
       "              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn2): DetrFrozenBatchNorm2d()\n",
       "              (drop_block): Identity()\n",
       "              (act2): ReLU(inplace=True)\n",
       "              (aa): Identity()\n",
       "              (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn3): DetrFrozenBatchNorm2d()\n",
       "              (act3): ReLU(inplace=True)\n",
       "            )\n",
       "            (16): Bottleneck(\n",
       "              (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn1): DetrFrozenBatchNorm2d()\n",
       "              (act1): ReLU(inplace=True)\n",
       "              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn2): DetrFrozenBatchNorm2d()\n",
       "              (drop_block): Identity()\n",
       "              (act2): ReLU(inplace=True)\n",
       "              (aa): Identity()\n",
       "              (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn3): DetrFrozenBatchNorm2d()\n",
       "              (act3): ReLU(inplace=True)\n",
       "            )\n",
       "            (17): Bottleneck(\n",
       "              (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn1): DetrFrozenBatchNorm2d()\n",
       "              (act1): ReLU(inplace=True)\n",
       "              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn2): DetrFrozenBatchNorm2d()\n",
       "              (drop_block): Identity()\n",
       "              (act2): ReLU(inplace=True)\n",
       "              (aa): Identity()\n",
       "              (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn3): DetrFrozenBatchNorm2d()\n",
       "              (act3): ReLU(inplace=True)\n",
       "            )\n",
       "            (18): Bottleneck(\n",
       "              (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn1): DetrFrozenBatchNorm2d()\n",
       "              (act1): ReLU(inplace=True)\n",
       "              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn2): DetrFrozenBatchNorm2d()\n",
       "              (drop_block): Identity()\n",
       "              (act2): ReLU(inplace=True)\n",
       "              (aa): Identity()\n",
       "              (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn3): DetrFrozenBatchNorm2d()\n",
       "              (act3): ReLU(inplace=True)\n",
       "            )\n",
       "            (19): Bottleneck(\n",
       "              (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn1): DetrFrozenBatchNorm2d()\n",
       "              (act1): ReLU(inplace=True)\n",
       "              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn2): DetrFrozenBatchNorm2d()\n",
       "              (drop_block): Identity()\n",
       "              (act2): ReLU(inplace=True)\n",
       "              (aa): Identity()\n",
       "              (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn3): DetrFrozenBatchNorm2d()\n",
       "              (act3): ReLU(inplace=True)\n",
       "            )\n",
       "            (20): Bottleneck(\n",
       "              (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn1): DetrFrozenBatchNorm2d()\n",
       "              (act1): ReLU(inplace=True)\n",
       "              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn2): DetrFrozenBatchNorm2d()\n",
       "              (drop_block): Identity()\n",
       "              (act2): ReLU(inplace=True)\n",
       "              (aa): Identity()\n",
       "              (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn3): DetrFrozenBatchNorm2d()\n",
       "              (act3): ReLU(inplace=True)\n",
       "            )\n",
       "            (21): Bottleneck(\n",
       "              (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn1): DetrFrozenBatchNorm2d()\n",
       "              (act1): ReLU(inplace=True)\n",
       "              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn2): DetrFrozenBatchNorm2d()\n",
       "              (drop_block): Identity()\n",
       "              (act2): ReLU(inplace=True)\n",
       "              (aa): Identity()\n",
       "              (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn3): DetrFrozenBatchNorm2d()\n",
       "              (act3): ReLU(inplace=True)\n",
       "            )\n",
       "            (22): Bottleneck(\n",
       "              (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn1): DetrFrozenBatchNorm2d()\n",
       "              (act1): ReLU(inplace=True)\n",
       "              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn2): DetrFrozenBatchNorm2d()\n",
       "              (drop_block): Identity()\n",
       "              (act2): ReLU(inplace=True)\n",
       "              (aa): Identity()\n",
       "              (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn3): DetrFrozenBatchNorm2d()\n",
       "              (act3): ReLU(inplace=True)\n",
       "            )\n",
       "          )\n",
       "          (layer4): Sequential(\n",
       "            (0): Bottleneck(\n",
       "              (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn1): DetrFrozenBatchNorm2d()\n",
       "              (act1): ReLU(inplace=True)\n",
       "              (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "              (bn2): DetrFrozenBatchNorm2d()\n",
       "              (drop_block): Identity()\n",
       "              (act2): ReLU(inplace=True)\n",
       "              (aa): Identity()\n",
       "              (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn3): DetrFrozenBatchNorm2d()\n",
       "              (act3): ReLU(inplace=True)\n",
       "              (downsample): Sequential(\n",
       "                (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "                (1): DetrFrozenBatchNorm2d()\n",
       "              )\n",
       "            )\n",
       "            (1): Bottleneck(\n",
       "              (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn1): DetrFrozenBatchNorm2d()\n",
       "              (act1): ReLU(inplace=True)\n",
       "              (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn2): DetrFrozenBatchNorm2d()\n",
       "              (drop_block): Identity()\n",
       "              (act2): ReLU(inplace=True)\n",
       "              (aa): Identity()\n",
       "              (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn3): DetrFrozenBatchNorm2d()\n",
       "              (act3): ReLU(inplace=True)\n",
       "            )\n",
       "            (2): Bottleneck(\n",
       "              (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn1): DetrFrozenBatchNorm2d()\n",
       "              (act1): ReLU(inplace=True)\n",
       "              (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn2): DetrFrozenBatchNorm2d()\n",
       "              (drop_block): Identity()\n",
       "              (act2): ReLU(inplace=True)\n",
       "              (aa): Identity()\n",
       "              (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn3): DetrFrozenBatchNorm2d()\n",
       "              (act3): ReLU(inplace=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (position_embedding): DetrSinePositionEmbedding()\n",
       "    )\n",
       "    (input_projection): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (query_position_embeddings): Embedding(100, 256)\n",
       "    (encoder): DetrEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-5): 6 x DetrEncoderLayer(\n",
       "          (self_attn): DetrAttention(\n",
       "            (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation_fn): ReLU()\n",
       "          (fc1): Linear(in_features=256, out_features=2048, bias=True)\n",
       "          (fc2): Linear(in_features=2048, out_features=256, bias=True)\n",
       "          (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (decoder): DetrDecoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-5): 6 x DetrDecoderLayer(\n",
       "          (self_attn): DetrAttention(\n",
       "            (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "          (activation_fn): ReLU()\n",
       "          (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): DetrAttention(\n",
       "            (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=256, out_features=2048, bias=True)\n",
       "          (fc2): Linear(in_features=2048, out_features=256, bias=True)\n",
       "          (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layernorm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (class_labels_classifier): Linear(in_features=256, out_features=18, bias=True)\n",
       "  (bbox_predictor): DetrMLPPredictionHead(\n",
       "    (layers): ModuleList(\n",
       "      (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)\n",
       "      (2): Linear(in_features=256, out_features=4, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import DetrForObjectDetection,DetrImageProcessor\n",
    "\n",
    "# settings\n",
    "DEVICE = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "CONFIDENCE_TRESHOLD = 0.1\n",
    "IOU_TRESHOLD = 0.7\n",
    "\n",
    "model_path=HOME/\"outputs/detr/detr_cocass/detr_cocass_v2\"\n",
    "\n",
    "CHECKPOINT = 'facebook/detr-resnet-101'\n",
    "\n",
    "image_processor = DetrImageProcessor.from_pretrained(CHECKPOINT)\n",
    "model=DetrForObjectDetection.from_pretrained(model_path)    # load the model from the checkpoint as a DEtrForObjectDetection model pretrained by the weights of the model_path\n",
    "#model = Detr(model)\n",
    "model.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sahi.models.huggingface.HuggingfaceDetectionModel"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detr_detection_model = AutoDetectionModel.from_pretrained(\n",
    "    model_type=\"huggingface\",\n",
    "    #config_path=\"../../resources/models/detr/detr_coco7_2800/config.json\",\n",
    "    model=model,\n",
    "    #model_path=model_path,\n",
    "    processor=image_processor,\n",
    "    confidence_threshold=0.7,\n",
    "    device=\"cuda:0\", # or 'cuda:0'\n",
    ")\n",
    "type(detr_detection_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detectron2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../models/detectron2/config4.yaml not available in Model Zoo!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "09/04/2024 13:33:07 - INFO - detectron2.checkpoint.detection_checkpoint -   [DetectionCheckpointer] Loading from ../outputs/detectron2/faster_rcnn_v3.pth ...\n",
      "09/04/2024 13:33:07 - INFO - fvcore.common.checkpoint -   [Checkpointer] Loading from ../outputs/detectron2/faster_rcnn_v3.pth ...\n",
      "09/04/2024 13:33:07 - WARNING - sahi.models.detectron2 -   Attribute 'thing_classes' does not exist in the metadata of dataset 'my_dataset_train': metadata is empty.\n"
     ]
    }
   ],
   "source": [
    "model_path = \"../outputs/detectron2/faster_rcnn_v3.pth\"\n",
    "det2_detection_model = AutoDetectionModel.from_pretrained(\n",
    "    model_type='detectron2',\n",
    "    #model=predictor,\n",
    "    model_path=model_path,\n",
    "    config_path=\"../models/detectron2/config4.yaml\",\n",
    "    #category_remapping= {i: class_name for i, class_name in enumerate(dataset.get_classes('annotations'))},\n",
    "    confidence_threshold=0.1,\n",
    "    image_size=1280,\n",
    "    device=\"cuda:0\", # or 'cuda:0'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Dataset\n",
    "\n",
    "We use fiftyone to iterate over the samples of the test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 100% |█████████████████████| 0/0 [741.9us elapsed, ? remaining, ? samples/s] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "09/03/2024 13:56:29 - INFO - eta.core.utils -    100% |█████████████████████| 0/0 [741.9us elapsed, ? remaining, ? samples/s] \n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "Sample field 'detections' does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 10\u001b[0m\n\u001b[0;32m      6\u001b[0m dataset\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Les annotations sont nommées en tant que détections, nous les renommons\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m#pour éviter les confusions\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m \u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrename_sample_field\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdetections\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mannotations\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Geomatique\\Documents\\map-symbols-detection-in-historical-maps\\ehess_env\\Lib\\site-packages\\fiftyone\\core\\dataset.py:1629\u001b[0m, in \u001b[0;36mDataset.rename_sample_field\u001b[1;34m(self, field_name, new_field_name)\u001b[0m\n\u001b[0;32m   1619\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrename_sample_field\u001b[39m(\u001b[38;5;28mself\u001b[39m, field_name, new_field_name):\n\u001b[0;32m   1620\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Renames the sample field to the given new name.\u001b[39;00m\n\u001b[0;32m   1621\u001b[0m \n\u001b[0;32m   1622\u001b[0m \u001b[38;5;124;03m    You can use dot notation (``embedded.field.name``) to rename embedded\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1627\u001b[0m \u001b[38;5;124;03m        new_field_name: the new field name or ``embedded.field.name``\u001b[39;00m\n\u001b[0;32m   1628\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1629\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_rename_sample_fields\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[43mfield_name\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mnew_field_name\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Geomatique\\Documents\\map-symbols-detection-in-historical-maps\\ehess_env\\Lib\\site-packages\\fiftyone\\core\\dataset.py:1671\u001b[0m, in \u001b[0;36mDataset._rename_sample_fields\u001b[1;34m(self, field_mapping, view)\u001b[0m\n\u001b[0;32m   1668\u001b[0m sample_collection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m view \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m view\n\u001b[0;32m   1670\u001b[0m paths, new_paths \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mfield_mapping\u001b[38;5;241m.\u001b[39mitems())\n\u001b[1;32m-> 1671\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sample_doc_cls\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_rename_fields\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1672\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_collection\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpaths\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnew_paths\u001b[49m\n\u001b[0;32m   1673\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1675\u001b[0m fields, _, _, _ \u001b[38;5;241m=\u001b[39m _parse_field_mapping(field_mapping)\n\u001b[0;32m   1677\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fields:\n",
      "File \u001b[1;32mc:\\Users\\Geomatique\\Documents\\map-symbols-detection-in-historical-maps\\ehess_env\\Lib\\site-packages\\fiftyone\\core\\odm\\mixins.py:433\u001b[0m, in \u001b[0;36mDatasetMixin._rename_fields\u001b[1;34m(cls, sample_collection, paths, new_paths)\u001b[0m\n\u001b[0;32m    430\u001b[0m existing_field \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_get_field(new_path, allow_missing\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    432\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m field \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m is_root_field:\n\u001b[1;32m--> 433\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[0;32m    434\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m field \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m does not exist\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_doc_name(), path)\n\u001b[0;32m    435\u001b[0m     )\n\u001b[0;32m    437\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_default:\n\u001b[0;32m    438\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    439\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot rename default \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m field \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    440\u001b[0m         \u001b[38;5;241m%\u001b[39m (\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_doc_name()\u001b[38;5;241m.\u001b[39mlower(), path)\n\u001b[0;32m    441\u001b[0m     )\n",
      "\u001b[1;31mAttributeError\u001b[0m: Sample field 'detections' does not exist"
     ]
    }
   ],
   "source": [
    "# Importation of the dataset\n",
    "dataset = fo.Dataset.from_dir(\n",
    "    data_path=HOME/\"data/coco_datasets/Cocass_aug/images\",  # path to the dataset\n",
    "    labels_path=HOME/\"data/coco_datasets/Cocass_aug/fraw_detailed_val.json\",   # path to the labels/annotations file\n",
    "    dataset_type=fo.types.COCODetectionDataset, # the type of dataset to import\n",
    ")\n",
    "dataset\n",
    "\n",
    "# the annotations are imported as \"detections\" but we want to rename them as \"annotations\" to avoid confusion\n",
    "dataset.rename_sample_field(\"detections\", \"annotations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Session launched. Run `session.show()` to open the App in a cell output.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "08/30/2024 10:33:14 - INFO - fiftyone.core.session.session -   Session launched. Run `session.show()` to open the App in a cell output.\n"
     ]
    },
    {
     "data": {
      "application/javascript": "window.open('http://localhost:5151/');",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# To inspet the dataset on an external app\n",
    "session = fo.launch_app(dataset, auto=False)\n",
    "session.open_tab()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the the dataset imported is made of crops (if not skip this kernel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 100% |█████████████████| 997/997 [5.0m elapsed, 0s remaining, 3.1 samples/s]      \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "09/04/2024 11:22:33 - INFO - eta.core.utils -    100% |█████████████████| 997/997 [5.0m elapsed, 0s remaining, 3.1 samples/s]      \n"
     ]
    }
   ],
   "source": [
    "remapping={i: class_name for i, class_name in enumerate(dataset.get_classes('annotations'))}    # remapping of the labels for faster R-CNN (sometime the label are not kept into the configs)\n",
    "label_set ={'0', '1', '10', '11', '13','14', '15', '2', '3', '4', '5', '6', '7', '8', '9'}\n",
    "kwargs={\"slice_height\":1280, \"slice_width\":1280, \"overlap_height_ratio\":0.9, \"overlap_width_ratio\":0.9}\n",
    "\n",
    "for sample in dataset.iter_samples(progress=True, autosave=True):\n",
    "    yolo_results=fo_predict_simple(yolo_detection_model,sample, \n",
    "        label_field=\"yolo_predictions\", \n",
    "        kwargs=kwargs)\n",
    "    # detr_results=fo_predict_simple(detr_detection_model,sample, \n",
    "    #     label_field=\"detr_predictions\", \n",
    "    #     kwargs=kwargs)      \n",
    "    # fo_predict_simple(det2_detection_model,sample, \n",
    "    #     label_field=\"det2_predictions\", \n",
    "    #     kwargs=kwargs)\n",
    "    # for det in sample[\"det2_predictions\"].detections:\n",
    "    #     det.label = remapping[int(det.label)]    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the dataset is only a full page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.predictions import fo_predict_with_slicing\n",
    "remapping={i: class_name for i, class_name in enumerate(dataset.get_classes('annotations'))}\n",
    "label_set ={'0', '1', '10', '11', '13','14', '15', '2', '3', '4', '5', '6', '7', '8', '9'}\n",
    "kwargs={\"slice_height\":1280, \"slice_width\":1280, \"overlap_height_ratio\":0.9, \"overlap_width_ratio\":0.9}\n",
    "# We use Sliced Aided Hyper Inference to predict on the dataset\n",
    "for sample in dataset.iter_samples(progress=True, autosave=True):\n",
    "    yolo_results=fo_predict_with_slicing(yolo_detection_model,sample, \n",
    "        label_field=\"yolo_predictions\", \n",
    "        slice_height=640, slice_width=640, overlap_height_ratio=0.8, overlap_width_ratio=0.8)\n",
    "    detr_results=fo_predict_with_slicing(detr_detection_model,sample, \n",
    "        label_field=\"detr_predictions\", \n",
    "        slice_height=640, slice_width=640, overlap_height_ratio=0.8, overlap_width_ratio=0.8)      \n",
    "    fo_predict_with_slicing(det2_detection_model,sample, \n",
    "        label_field=\"det2_predictions\", \n",
    "        slice_height=640, slice_width=640, overlap_height_ratio=0.8, overlap_width_ratio=0.8)\n",
    "    for det in sample[\"det2_predictions\"].detections:\n",
    "        det.label = remapping[int(det.label)]   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This part as been added to export the results in COCO format in json to filter them and return them on fiftyone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory 'results/jsons/f9/yolo_predictions' already exists; export will be merged with existing files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "09/04/2024 11:22:33 - WARNING - fiftyone.core.collections -   Directory 'results/jsons/f9/yolo_predictions' already exists; export will be merged with existing files\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 100% |█████████████████| 997/997 [7.2s elapsed, 0s remaining, 73.1 samples/s]       \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "09/04/2024 11:22:40 - INFO - eta.core.utils -    100% |█████████████████| 997/997 [7.2s elapsed, 0s remaining, 73.1 samples/s]       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image IDs and annotations have been remapped successfully.\n",
      "Annotations in the second JSON file have been updated successfully.\n",
      "Filtering annotations...\n",
      "Annotations have been filtered successfully.\n",
      "Garbage annotations have been saved successfully.\n",
      " 100% |█████████████████| 997/997 [8.9s elapsed, 0s remaining, 57.7 samples/s]       \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "09/04/2024 11:23:24 - INFO - eta.core.utils -    100% |█████████████████| 997/997 [8.9s elapsed, 0s remaining, 57.7 samples/s]       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image IDs and annotations have been remapped successfully.\n",
      "Annotations in the second JSON file have been updated successfully.\n",
      "Filtering annotations...\n",
      "Annotations have been filtered successfully.\n",
      "Garbage annotations have been saved successfully.\n",
      " 100% |█████████████████| 997/997 [11.4s elapsed, 0s remaining, 49.3 samples/s]      \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "09/04/2024 11:24:25 - INFO - eta.core.utils -    100% |█████████████████| 997/997 [11.4s elapsed, 0s remaining, 49.3 samples/s]      \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image IDs and annotations have been remapped successfully.\n",
      "Annotations in the second JSON file have been updated successfully.\n",
      "Filtering annotations...\n",
      "Annotations have been filtered successfully.\n",
      "Garbage annotations have been saved successfully.\n"
     ]
    }
   ],
   "source": [
    "from utils.data_cleaning import fiftyone_extraction_remapping,filter_annotations\n",
    "import fiftyone.utils.coco as fouc\n",
    "import json\n",
    "\n",
    "# YOLO predictions filtering\n",
    "# ------------------------\n",
    "# Export the dataset to a JSON file\n",
    "gt_json = HOME/\"data/coco_datasets/Cocass_aug/fraw_detailed_val.json\"\n",
    "gt_json = HOME/\"data/coco_datasets/tests/Cassini_009_LoC/f009_detailed_updated.json\"\n",
    "dataset.export(\n",
    "    export_dir=\"results/jsons/f9/yolo_predictions\",\n",
    "    data_path=HOME/\"data/coco_datasets/Cocass_aug/images\",\n",
    "    dataset_type=fo.types.COCODetectionDataset,  # You can choose other formats as well\n",
    "    label_field=\"yolo_predictions\",\n",
    "    classes=dataset.get_classes('annotations'),\n",
    "    export_media=False  # Set to True if you want to export media files as well\n",
    ")\n",
    "fiftyone_extraction_remapping(HOME/\"notebooks/results/jsons/f9/yolo_predictions/labels.json\",\n",
    "                              gt_json,inplace=True)\n",
    "filter_annotations(json_file_path=HOME/\"notebooks/results/jsons/f9/yolo_predictions/labels.json\",conf_threshold=0.5,\n",
    "                    height_width_ratio= 0.3,\n",
    "                    second_height_width_ratio=3,\n",
    "                    iou_threshold= 0.8,inside_threshold=0.2,inplace=False,keep_annotations=True,class_agnostic=False)\n",
    "fouc.add_coco_labels(\n",
    "    dataset,\n",
    "    \"yolo_predictions_filtered\",\n",
    "    (HOME/\"notebooks/results/jsons/f9/yolo_predictions/filtered_labels.json\").as_posix(),\n",
    "    classes=dataset.get_classes('annotations'),\n",
    ")\n",
    "\n",
    "# DETR predictions filtering\n",
    "# ------------------------\n",
    "dataset.export(\n",
    "    export_dir=\"results/jsons/f9/detr_predictions\",\n",
    "    dataset_type=fo.types.COCODetectionDataset,  # You can choose other formats as well\n",
    "    label_field=\"detr_predictions\",\n",
    "    # classes=dataset.get_classes('annotations'),\n",
    "    export_media=False  # Set to True if you want to export media files as well\n",
    ")\n",
    "fiftyone_extraction_remapping(HOME/\"notebooks/results/jsons/f9/detr_predictions/labels.json\",\n",
    "                              gt_json,inplace=True)\n",
    "filter_annotations(json_file_path=HOME/\"notebooks/results/jsons/f9/detr_predictions/labels.json\",conf_threshold=0.5,\n",
    "                    height_width_ratio= 0.3,\n",
    "                    second_height_width_ratio=3,\n",
    "                    iou_threshold= 0.8,inside_threshold=0.2,inplace=False,keep_annotations=True,class_agnostic=False)\n",
    "fouc.add_coco_labels(\n",
    "    dataset,\n",
    "    \"detr_predictions_filtered\",\n",
    "    (HOME/\"notebooks/results/jsons/f9/detr_predictions/filtered_labels.json\").as_posix(),\n",
    "    classes=dataset.get_classes('annotations'),\n",
    ")\n",
    "\n",
    "# DET2 predictions filtering\n",
    "# ------------------------\n",
    "dataset.export(\n",
    "    export_dir=\"results/jsons/f9/det2_predictions\",\n",
    "    dataset_type=fo.types.COCODetectionDataset,  # You can choose other formats as well\n",
    "    label_field=\"det2_predictions\",\n",
    "    # classes=dataset.get_classes('annotations'),\n",
    "    export_media=False  # Set to True if you want to export media files as well\n",
    ")\n",
    "fiftyone_extraction_remapping(HOME/\"notebooks/results/jsons/f9/det2_predictions/labels.json\",\n",
    "                              gt_json,inplace=True)\n",
    "filter_annotations(json_file_path=HOME/\"notebooks/results/jsons/f9/det2_predictions/labels.json\",conf_threshold=0.5,\n",
    "                    height_width_ratio= 0.3,\n",
    "                    second_height_width_ratio=3,\n",
    "                    iou_threshold= 0.8,inside_threshold=0.2,inplace=False,keep_annotations=True,class_agnostic=False)\n",
    "fouc.add_coco_labels(\n",
    "    dataset,\n",
    "    \"det2_predictions_filtered\",\n",
    "    (HOME/\"notebooks/results/jsons/f9/det2_predictions/filtered_labels.json\").as_posix(),\n",
    "    classes=dataset.get_classes('annotations'),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation of the model (we use the base fiftyone function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating detections...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "08/30/2024 10:36:49 - INFO - fiftyone.utils.eval.detection -   Evaluating detections...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 100% |█████████████████| 997/997 [50.0s elapsed, 0s remaining, 6.4 samples/s]       \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "08/30/2024 10:37:39 - INFO - eta.core.utils -    100% |█████████████████| 997/997 [50.0s elapsed, 0s remaining, 6.4 samples/s]       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing IoU sweep...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "08/30/2024 10:37:39 - INFO - fiftyone.utils.eval.coco -   Performing IoU sweep...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 100% |█████████████████| 997/997 [24.1s elapsed, 0s remaining, 14.3 samples/s]      \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "08/30/2024 10:38:03 - INFO - eta.core.utils -    100% |█████████████████| 997/997 [24.1s elapsed, 0s remaining, 14.3 samples/s]      \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating detections...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "08/30/2024 10:38:03 - INFO - fiftyone.utils.eval.detection -   Evaluating detections...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 100% |█████████████████| 997/997 [54.0s elapsed, 0s remaining, 4.9 samples/s]       \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "08/30/2024 10:38:57 - INFO - eta.core.utils -    100% |█████████████████| 997/997 [54.0s elapsed, 0s remaining, 4.9 samples/s]       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing IoU sweep...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "08/30/2024 10:38:57 - INFO - fiftyone.utils.eval.coco -   Performing IoU sweep...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 100% |█████████████████| 997/997 [32.9s elapsed, 0s remaining, 15.1 samples/s]      \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "08/30/2024 10:39:30 - INFO - eta.core.utils -    100% |█████████████████| 997/997 [32.9s elapsed, 0s remaining, 15.1 samples/s]      \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating detections...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "08/30/2024 10:39:30 - INFO - fiftyone.utils.eval.detection -   Evaluating detections...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 100% |█████████████████| 997/997 [1.0m elapsed, 0s remaining, 4.4 samples/s]        \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "08/30/2024 10:40:31 - INFO - eta.core.utils -    100% |█████████████████| 997/997 [1.0m elapsed, 0s remaining, 4.4 samples/s]        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing IoU sweep...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "08/30/2024 10:40:31 - INFO - fiftyone.utils.eval.coco -   Performing IoU sweep...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 100% |█████████████████| 997/997 [34.0s elapsed, 0s remaining, 10.7 samples/s]      \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "08/30/2024 10:41:05 - INFO - eta.core.utils -    100% |█████████████████| 997/997 [34.0s elapsed, 0s remaining, 10.7 samples/s]      \n"
     ]
    }
   ],
   "source": [
    "yolo_results = dataset.evaluate_detections(\n",
    "    \"yolo_predictions_filtered\",\n",
    "    gt_field=\"annotations\",\n",
    "    eval_key=\"yolo_eval\",\n",
    "    compute_mAP=True,\n",
    ")\n",
    "\n",
    "detr_results = dataset.evaluate_detections(\n",
    "    \"detr_predictions_filtered\",\n",
    "    gt_field=\"annotations\",\n",
    "    eval_key=\"detr_eval\",\n",
    "    compute_mAP=True,\n",
    ")\n",
    "\n",
    "det2_results = dataset.evaluate_detections(\n",
    "    \"det2_predictions_filtered\",\n",
    "    gt_field=\"annotations\",\n",
    "    eval_key=\"det2_eval\",\n",
    "    compute_mAP=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " YOLO evaluation on test dataset :\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "              hameau       0.91      0.86      0.89      5083\n",
      "        moulin_a_eau       0.91      0.62      0.74      1558\n",
      "             clocher       0.81      0.69      0.74       506\n",
      "             chateau       0.85      0.54      0.66       567\n",
      "              abbaye       0.67      0.11      0.18        56\n",
      "              maison       0.93      0.79      0.85     13337\n",
      "            chapelle       0.73      0.32      0.44       215\n",
      "               bourg       0.91      0.69      0.79        62\n",
      "calvaire_ou_oratoire       0.88      0.13      0.23        52\n",
      "      gentilhommiere       0.00      0.00      0.00        39\n",
      "             justice       0.00      0.00      0.00        21\n",
      "               ville       0.62      0.22      0.33        36\n",
      "              cabane       0.00      0.00      0.00         0\n",
      "             prieure       1.00      0.17      0.30        40\n",
      "       moulin_a_vent       0.44      0.59      0.50        66\n",
      "                fort       0.00      0.00      0.00         0\n",
      "               autre       0.00      0.00      0.00         0\n",
      "\n",
      "           micro avg       0.92      0.77      0.84     21638\n",
      "           macro avg       0.57      0.34      0.39     21638\n",
      "        weighted avg       0.91      0.77      0.83     21638\n",
      "\n",
      "YOLO mAP : 0.22863442593887023\n",
      " DETR evaluation on test dataset :\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "              hameau       0.87      0.73      0.79      5083\n",
      "        moulin_a_eau       0.84      0.58      0.68      1558\n",
      "             clocher       0.65      0.71      0.68       506\n",
      "             chateau       0.50      0.27      0.35       567\n",
      "              abbaye       0.00      0.00      0.00        56\n",
      "              maison       0.79      0.84      0.81     13337\n",
      "            chapelle       0.39      0.21      0.27       215\n",
      "               bourg       0.68      0.55      0.61        62\n",
      "calvaire_ou_oratoire       0.20      0.02      0.04        52\n",
      "      gentilhommiere       0.00      0.00      0.00        39\n",
      "             justice       0.33      0.10      0.15        21\n",
      "               ville       0.50      0.22      0.31        36\n",
      "              cabane       0.00      0.00      0.00         0\n",
      "             prieure       0.00      0.00      0.00        40\n",
      "       moulin_a_vent       0.37      0.47      0.42        66\n",
      "                fort       0.00      0.00      0.00         0\n",
      "               autre       0.00      0.00      0.00         0\n",
      "\n",
      "           micro avg       0.80      0.76      0.78     21638\n",
      "           macro avg       0.36      0.28      0.30     21638\n",
      "        weighted avg       0.79      0.76      0.77     21638\n",
      "\n",
      "DETR mAP : 0.14529571298667293\n",
      " Detectron2 evaluation on test dataset :\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "              hameau       0.89      0.79      0.84      5083\n",
      "        moulin_a_eau       0.91      0.54      0.68      1558\n",
      "             clocher       0.71      0.69      0.70       506\n",
      "             chateau       0.70      0.37      0.48       567\n",
      "              abbaye       0.09      0.02      0.03        56\n",
      "              maison       0.89      0.78      0.83     13337\n",
      "            chapelle       0.57      0.32      0.41       215\n",
      "               bourg       0.74      0.77      0.76        62\n",
      "calvaire_ou_oratoire       0.24      0.08      0.12        52\n",
      "      gentilhommiere       0.36      0.10      0.16        39\n",
      "             justice       0.00      0.00      0.00        21\n",
      "               ville       0.33      0.08      0.13        36\n",
      "              cabane       0.00      0.00      0.00         0\n",
      "             prieure       1.00      0.03      0.05        40\n",
      "       moulin_a_vent       0.47      0.35      0.40        66\n",
      "                fort       0.00      0.00      0.00         0\n",
      "               autre       0.00      0.00      0.00         0\n",
      "\n",
      "           micro avg       0.88      0.74      0.80     21638\n",
      "           macro avg       0.47      0.29      0.33     21638\n",
      "        weighted avg       0.87      0.74      0.80     21638\n",
      "\n",
      "Detectron2 mAP : 0.19780013213714653\n"
     ]
    }
   ],
   "source": [
    "print(\" YOLO evaluation on test dataset :\")\n",
    "yolo_results.print_report(classes=dataset.get_classes('annotations'))\n",
    "print(\"YOLO mAP :\",yolo_results.mAP())\n",
    "\n",
    "print(\" DETR evaluation on test dataset :\")\n",
    "detr_results.print_report(classes=dataset.get_classes('annotations'))\n",
    "print(\"DETR mAP :\",detr_results.mAP())\n",
    "\n",
    "print(\" Detectron2 evaluation on test dataset :\")\n",
    "det2_results.print_report(classes=dataset.get_classes('annotations'))\n",
    "print(\"Detectron2 mAP :\",det2_results.mAP())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataframes\n",
    "This part can be used to export dataframes from the results (in csv, json or xlsx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.benchmarking import fo_result2pd\n",
    "yolo_df_9 = fo_result2pd(yolo_results,file_path=\"results/f9/filtered_yolo_results.json\",save_json=True)\n",
    "detr_df_9 = fo_result2pd(detr_results,file_path=\"results/f9/filtered_detr_results.json\",save_json=True)\n",
    "det2_df_9 = fo_result2pd(det2_results,file_path=\"results/f9/fildet2_results.json\",save_json=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion matrices\n",
    "Those function takes as inputs the json exported from fiftyone. You may also use a built-in function from fiftyone but i find it hard to read and difficult to export."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.benchmarking import fo_plot_confusion_matrix\n",
    "plot = fo_plot_confusion_matrix(yolo_results,normalize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.benchmarking import  build_confusion_matrix, plot_confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import defaultdict\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def load_coco_json(file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    return data\n",
    "def iou(box1, box2):\n",
    "    \"\"\"Compute the Intersection over Union (IoU) of two bounding boxes.\"\"\"\n",
    "    x1, y1, w1, h1 = box1\n",
    "    x2, y2, w2, h2 = box2\n",
    "    \n",
    "    xi1 = max(x1, x2)\n",
    "    yi1 = max(y1, y2)\n",
    "    xi2 = min(x1 + w1, x2 + w2)\n",
    "    yi2 = min(y1 + h1, y2 + h2)\n",
    "    \n",
    "    inter_area = max(0, xi2 - xi1) * max(0, yi2 - yi1)\n",
    "    box1_area = w1 * h1\n",
    "    box2_area = w2 * h2\n",
    "    union_area = box1_area + box2_area - inter_area\n",
    "    \n",
    "    iou = inter_area / union_area\n",
    "    return iou\n",
    "\n",
    "def build_confusion_matrix(gt_json, pred_json, iou_threshold=0.5):\n",
    "    gt_data = load_coco_json(gt_json)\n",
    "    pred_data = load_coco_json(pred_json)\n",
    "    \n",
    "    gt_annotations = gt_data['annotations']\n",
    "    pred_annotations = pred_data['annotations']\n",
    "    \n",
    "    categories = {cat['id']: cat['name'] for cat in gt_data['categories']}\n",
    "    num_categories = len(categories)\n",
    "    gt_images = {img['id']: img for img in gt_data['images']}\n",
    "    gt_images_names={img['file_name']: img for img in gt_data['images']}\n",
    "    pred_images_names={img['file_name']: img for img in pred_data['images']}\n",
    "    pred_images = {img['id']: img for img in pred_data['images']}\n",
    "    gt_category_ids = set([cat['id'] for cat in gt_data['categories']])\n",
    "    # Add the background category\n",
    "    background_id = num_categories + 1\n",
    "    categories[background_id] = \"background\"\n",
    "    \n",
    "    # Initialize confusion matrix with an extra row/column for the background\n",
    "    matrix = np.zeros((num_categories + 1, num_categories + 1))\n",
    "    \n",
    "    for gt in gt_annotations:\n",
    "        matched = False\n",
    "        gt_image_file = gt_images[gt['image_id']]['file_name'] \n",
    "        for pred in pred_annotations:\n",
    "            pred_image_file = pred_images[pred['image_id']]['file_name']\n",
    "            if gt_image_file == pred_image_file:\n",
    "                # get the heigth and width of the image of corresponding name but gt_images as ids for keys\n",
    "\n",
    "                height,width = gt_images_names[gt_image_file]['height'],gt_images_names[gt_image_file]['width']\n",
    "\n",
    "                gt_box = gt['bbox']\n",
    "                # gt_box[0], gt_box[2] = gt_box[0] * width, gt_box[2] * width\n",
    "                # gt_box[1], gt_box[3] = gt_box[1] * height, gt_box[3] * height\n",
    "                gt_category_id = gt['category_id']\n",
    "\n",
    "                pred_box = pred['bbox']\n",
    "                # pred_box[0], pred_box[2] = pred_box[0] * width, pred_box[2] * width\n",
    "                # pred_box[1], pred_box[3] = pred_box[1] * height, pred_box[3] * height\n",
    "                pred_category_id = pred['category_id']\n",
    "                \n",
    "                if iou(gt_box, pred_box) >= iou_threshold:\n",
    "                    matrix[gt_category_id-1][pred_category_id-1] += 1\n",
    "                    matched = True\n",
    "                    # print(f'{categories[gt_category_id]} - {categories[pred_category_id]}')\n",
    "                    break\n",
    "        \n",
    "        if not matched:\n",
    "            matrix[gt_category_id-1][background_id-1] += 1\n",
    "    \n",
    "    for pred in pred_annotations:\n",
    "        matched = False\n",
    "        pred_image_file = pred_images[pred['image_id']]['file_name']               \n",
    "        for gt in gt_annotations:\n",
    "            gt_image_file = gt_images[gt['image_id']]['file_name']\n",
    "            if gt_image_file == pred_image_file:\n",
    "                height,width = pred_images_names[pred_image_file]['height'],pred_images_names[pred_image_file]['width']\n",
    "\n",
    "                pred_box = pred['bbox']\n",
    "                # pred_box[0], pred_box[2] = pred_box[0] * width, pred_box[2] * width\n",
    "                # pred_box[1], pred_box[3] = pred_box[1] * height, pred_box[3] * height\n",
    "                pred_category_id = pred['category_id']\n",
    "\n",
    "                gt_box = gt['bbox']\n",
    "                # gt_box[0], gt_box[2] = gt_box[0] * width, gt_box[2] * width\n",
    "                # gt_box[1], gt_box[3] = gt_box[1] * height, gt_box[3] * height\n",
    "                gt_category_id = gt['category_id']        \n",
    "\n",
    "                if iou(gt_box, pred_box) >= iou_threshold:\n",
    "                    matched = True\n",
    "                    break\n",
    "        \n",
    "        if not matched:\n",
    "            matrix[background_id-1][pred_category_id-1] += 1\n",
    "    \n",
    "    # Sort the categories and reorder the matrix accordingly\n",
    "    sorted_category_ids = sorted(categories.keys())\n",
    "    reordered_matrix = np.zeros((len(sorted_category_ids), len(sorted_category_ids)))\n",
    "    \n",
    "    for i, gt_id in enumerate(sorted_category_ids):\n",
    "        for j, pred_id in enumerate(sorted_category_ids):\n",
    "            reordered_matrix[i, j] = matrix[gt_id-1 if gt_id != background_id else -1, pred_id-1 if pred_id != background_id else -1]\n",
    "    \n",
    "    # Update categories to match the sorted order\n",
    "    sorted_categories = {i+1: categories[sorted_category_ids[i]] for i in range(len(sorted_category_ids))}\n",
    "    \n",
    "    return reordered_matrix, sorted_categories\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ehess_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
